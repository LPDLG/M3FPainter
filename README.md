# M3FPainter
The automatic coloring of Chinese line drawings is a challenging task in the fields of computer graphics and computer vision. Due to the fact that all colors, textures, and shadows need to be generated based on abstract line drawings, this places extremely high demands on the model's understanding and generation capabilities. In addition, the pigments and techniques used in Chinese painting are unique and often lose their original color information during the historical transmission process. Meanwhile, the complex and intricate line structure further increases the difficulty of coloring. However, previous methods have been difficult to fully learn sufficient color information during the coloring process, and cannot achieve reasonable and coordinated coloring based on the structural layout of the line draft. To address these issues, we propose a diffusion model based on multimodal fusion guidance, abbreviated as M3FPainter. Specifically, first, we introduce text and images as input modalities to provide richer coloring guidance information. Secondly, we have designed a new text temporal modeling module aimed at better capturing the semantic associations between words in the text, thereby enhancing the model's understanding of line layout. Finally, we constructed a cross modal feature fusion module to establish semantic mapping relationships between text and images, providing high-quality multimodal guidance information for the coloring process. A large number of experimental results have shown that our method can effectively generate coloring results with advantages in both color realism and artistic style based on understanding the semantic content of line drafts, which is significantly better than existing methods.
